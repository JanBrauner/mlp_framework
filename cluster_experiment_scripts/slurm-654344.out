sending incremental file list
./
use 2 GPU(s)
  0%|          | 0/470 [00:00<?, ?it/s][('continue_from_epoch', '-2'), ('seed', '0'), ('task', 'classification'), ('use_gpu', 'True'), ('gpu_id', '0,1'), ('model_name', 'context_encoder'), ('kernel_size', '4'), ('num_layers_enc', '6'), ('num_channels_enc', '64'), ('num_channels_progression_enc', '[1, 1, 2, 4, 8]'), ('num_channels_bottleneck', '8192'), ('num_layers_dec', '6'), ('num_channels_dec', '64'), ('num_channels_progression_dec', '[8, 4, 2, 1, 1]'), ('dataset_name', 'DescribableTextures'), ('num_image_channels', '3'), ('image_height', '128'), ('image_width', '128'), ('normalisation', 'range-11'), ('scale_image', 'None'), ('data_format', 'autoencoding'), ('debug_mode', 'False'), ('num_workers', '4'), ('augment', 'False'), ('gamma_factor', '1'), ('rot_angle', '0'), ('translate_factor', '[0, 0]'), ('scale_factor', '1'), ('shear_angle', '0'), ('patch_mode', 'True'), ('patch_size', '[128, 128]'), ('patch_location_during_training', 'random'), ('patch_rejection_threshold', '10'), ('image_padding_mode', 'None'), ('mask_size', '[64, 64]'), ('batch_size', '8'), ('loss', 'cross_entropy'), ('num_epochs', '200'), ('learning_rate', '0.0002'), ('betas', '[0.5, 0.999]'), ('weight_decay_coefficient', '0'), ('anomaly_dataset_name', 'DTPathologicalIrreg1'), ('AD_patch_stride', '[10, 10]'), ('measure_of_anomaly', 'likelihood'), ('window_aggregation_method', 'max'), ('save_anomaly_maps', 'True'), ('AD_margins', '[128, 128]'), ('AD_batch_size', '50'), ('experiment_name', 'AE_DTD_r3_prob_patch_128_bn_8192___AD_window_max')]
Loading data from:  /disk/scratch/s1890594/data/DTPathologicalIrreg1
Loading data from:  /disk/scratch/s1890594/data/DTPathologicalIrreg1

Traceback (most recent call last):
  File "anomaly_detection.py", line 56, in <module>
    experiment.run_experiment()
  File "/mnt/mscteach_home/s1890594/mlp_framework/experiment_builder.py", line 484, in run_experiment
    anomaly_maps_current_batch = self.calculate_anomaly_maps(inputs, targets) 
  File "/mnt/mscteach_home/s1890594/mlp_framework/experiment_builder.py", line 651, in calculate_anomaly_maps
    anomaly_maps = F.cross_entropy(outputs, targets, reduction="none")  # pixelwise NLL, dimensions batch_size x channels x mask_height x mask_width. Softmax is included internally in cross_entropy
  File "/home/s1890594/miniconda3/envs/mlp/lib/python3.7/site-packages/torch/nn/functional.py", line 2056, in cross_entropy
    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)
  File "/home/s1890594/miniconda3/envs/mlp/lib/python3.7/site-packages/torch/nn/functional.py", line 1350, in log_softmax
    ret = input.log_softmax(dim)
RuntimeError: CUDA out of memory. Tried to allocate 2.34 GiB (GPU 0; 5.94 GiB total capacity; 3.02 GiB already allocated; 2.32 GiB free; 147.76 MiB cached)
